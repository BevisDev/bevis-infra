# ============================== Filebeat inputs ===============================
filebeat.inputs:
  - type: filestream
    id: app1-log
    enabled: true
    json.keys_under_root: true
    json.add_error_key: true
    json.overwrite_keys: true
    paths:
      - /var/logs/app1/*.log
    fields:
      index_prefix: "app1-log" # for elasticsearch
      topic_name: "app1_topic" # use kafka
    fields_under_root: true

  - type: filestream
    id: app2-log
    enabled: true
    json.keys_under_root: true
    json.add_error_key: true
    json.overwrite_keys: true
    paths:
      - /var/logs/app2/*.log
    fields:
      index_prefix: "app2-log" # for elasticsearch
      topic_name: "app2_topic" # use kafka
    fields_under_root: true

# local buffer tránh mất log khi Kafka down
queue.mem:
  events: 8192
  flush.min_events: 512
  flush.timeout: 5s

# ============================== Filebeat modules ==============================
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 3

# ================================== Outputs ===================================
# ================== ELASTICSEARCH ==================
output.elasticsearch:
  protocol: https
  hosts: ["127.0.0.1:9200"]
  ssl.certificate_authorities: ["/etc/filebeat/http_ca.crt"]
  api_key: "${ES_API_KEY}"
  loadbalance: true
  worker: 2
  bulk_max_size: 2048
  indices:
    - index: "%{[index_prefix]}-%{+YYYY.MM.dd}"

# ================== KAFKA==================
output.kafka:
  enabled: true
  hosts: ["kafka:29092"]
  topic: "%{[topic_name]}"
  required_acks: -1 # all replicas committed: không mất log
  worker: 2

# ================================== Logging ===================================
logging.level: error
logging.to_files: true
logging.files:
  path: /usr/share/filebeat/logs
  name: filebeat2
  keepfiles: 7
  permissions: 0644
seccomp:
  default_action: allow
